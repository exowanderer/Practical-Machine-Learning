{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks\n",
    "===\n",
    "\n",
    "NN are an old idea -- as old as the 1940s -- but have become revitalized recently because of the Deep Learning movement; it has been shown to break all possible tests that require data to be mapped into outputs -- non-parametric modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How Biological Neurons Work\n",
    "---\n",
    "\n",
    "Dendrites: filements that connect one neuron to the next\n",
    "\n",
    "Nucleus: core of neuron that containes genetic code (brain of cell)\n",
    "\n",
    "Axon: Shaft connecting to Nucleus\n",
    "\n",
    "Axon Terminal: end of Axon near dendrites\n",
    "\n",
    "Synapse: chemically rich space between dendrites where information is transferred from neuron to neuron\n",
    "\n",
    "An electrical signal is generated at the left, processed by the nucleous, and is or is not passed onto the next neuron or output!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How Artificial Neurons Work:\n",
    "---\n",
    "\n",
    "Take Input values (features/samples) get passed along, through a set of weighting functions, to be summed together into the next layer of neurons.\n",
    "\n",
    "A neuron, based on the input either fires or it does not fire\n",
    "- the summation gets passed through a threshold or step function\n",
    "- if the input summation is below the threshold, the threshold function results in a 0\n",
    "- if the input summation is above the threshold, the threshold function results in a 1\n",
    "- This is a perceptron\n",
    "\n",
    "After this neuron is activated or not, the output becomes the input of the next connected or hidden layer in the neural networks\n",
    "\n",
    "Note that typically people do not use an actual step function because {0,1} is too binary.  \n",
    "- they would like to have a little more dynamic range or flexibility in each layer\n",
    "- so people normally use a sigmoid function, which has a gradual step from 0 to 1\n",
    "- \"it is called sigmoid because it looks like an S\" -- is that True??\n",
    "\n",
    "Because the \"threshold\" function is no longer a literal \"theshold\", so now we call it the \"Activation\" function.\n",
    "\n",
    "This tends to break down because the output of each layer is a function of x's and w's:\n",
    "$$ y = f(\\bar{x},\\bar{w}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Neural Nets\n",
    "---\n",
    "\n",
    "\n",
    "That was just a single neuron, let's see what an actual neural network looks like:\n",
    "\n",
    "The input data is processed through the activation funcitons onto the next layer. This can happen via multiple neurons and activations functions, as deep and complex as desired. Then the output goes to the next layer, and the next layer, on and on, until the output layer, which is the prediction of the NN that is compared to the labels in a classification scheme.\n",
    "\n",
    "\n",
    "Deep networks are any neural network with more than 1 hidden layer. In the example image, we have 2 hidden layers and therefore is a **deep** neural network.\n",
    "\n",
    "---\n",
    "That's it! Why did it take so long to come to fruition?\n",
    "\n",
    "Mostly it took a long time for DNNs to become useful because they require *so* much data to process through them for accurate estimation, that the amount of data we had before was just not enough to make DNNs useful or accurate.\n",
    "\n",
    "In the first example we will cover, they have 60k samples; and that's not even 'big' by DNN standards. Most of the commerical examples out there that are doing 'crazy' things have more than 500M samples.  After about 500M samples, it appears -- with diminishing returns -- that you are not really seeing much better results; but that's about what it takes right now.\n",
    "\n",
    "Recall from the SVM, we had what was called a *convex* optimization problem.  That meant that to optimize the SVM hyperparameters, we only had to traverse in the constraint space along the tangents to the convex solution curves.\n",
    "\n",
    "But, with DNNs, the solution space (think *like* $\\chi^2$ space) we could have many bumps and wiggles, with respect to both time and parameters.\n",
    "\n",
    "In addition to the SVM being more simple to solve altogether, it only had 2 parameters that mattered: $\\textbf{w}$, and $b$.  But, with the DNN, even in the simple 2 HL example that we have above, there are 33 weights that all have to be optimized simultaneously; and they are *all* correlated to one another!\n",
    "- that's a lot of unique weights, and a lot of variables\n",
    "- it's a very challenging problem for both mathematics and computations to control for them\n",
    "\n",
    "Also, you need a lot data!  The more data you have, the more samples and therefore the more weights (\"right?\").\n",
    "\n",
    "So it was 'kind of like this' 'perfect storm' that stopped NNs from growing in popularity.\n",
    "- But these days we have both a lot of data *and* faster computers to process all of those data sets\n",
    "    -- 'perfect storm'\n",
    "- That's what really changed and allowed the DNNs to truly shine\n",
    "\n",
    "With respect to normal classification problems, *I* think that the DNNs perform about the same as other classifiers; for example the state of the art SVM could be maybe 97%, while the state of the art DNN could perform 98% or 99%.\n",
    "- That little tiny percentage is what everyone is fighting over right now\n",
    "- But to me that is not as impressive as what DNN are doing to the data;\n",
    "\n",
    "To *Sentdex*, what's more impressive is the modeling aspect of what DNNs are *doing* to the data.  People don't fully understand how the modeling with DNNs works, but the modeling does do very very very well.\n",
    "- There is a lot of digging and analysis to demystify what is going on in the DNN\n",
    "- But we won't have enough data to really understand that\n",
    "- \"it's just not possible for us\"\n",
    "\n",
    "For example, consider \"Jack is 12; Jane is 10; Kate is older than Jane and younger than Jack?\"\n",
    "    - Kate is 11\n",
    "- to get that answer, you have to employ a little bit of logic\n",
    "- MOST classification schemes to day are not able to understand logic; they are great at classifying, but not interpreting\n",
    "- Up until *very* recently, if you wanted to make an algorithm to answer then you would have had to build a machine to model the linguistics and to know those linguistics yourself.\n",
    "- Where as with a neural network, you don't!\n",
    "    - you take a bunch of examples like that and then just chug through (say 1M or 4M times) \n",
    "        and the NN can figure that out on it's own\n",
    "    - it learns how to model on its own\n",
    "\n",
    "That is what is so impressive! The output that we get and the methodology that we use is what's the most fascinating to me (and Sentdex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where to get this 4M data?\n",
    "---\n",
    "\n",
    "Images: imagenet\n",
    "\n",
    "Text: Wikipedia data dump, crawl reddit, twitter?\n",
    "\n",
    "Speech: tatoba (?)\n",
    "\n",
    "Common crawl: huge huge huge data set! \n",
    "    - pedabytes of parsed websites\n",
    "    \n",
    "Google and Facebook actually possess on their own servers enough data to make this work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages\n",
    "---\n",
    "\n",
    "We will be using TensorFlow because it's new -- beta at the time of this taping\n",
    "\n",
    "There is Keras / Theano / Torch, but they all pretty much do the same things\n",
    "    - they use sequential layers of weights to map the inputs to the outputs of the training data sets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Running Tensorflow\n",
    "---\n",
    "\n",
    "Tensorflow takes all of your inputs / labels, then runs in the background and returns with the answers. This is something in between Python and Compiled coding; but it's more like Compiled coding\n",
    "\n",
    "TF does have an interactive session, but you should build the code to be run at optimal usage, like the compiled versions\n",
    "\n",
    "DNN packages like Tensorflow, Theano, Keras, Caffe, Torch, etc, are just matrix processing libraries. They are just stacks of packages that can multiply matrices in fast ways.\n",
    "\n",
    "So \"what's a tensor?\" -- it's an array like value in multi-dimensions and scales\n",
    "\n",
    "All tensorflow does is process functions on a set of tensors\n",
    "\n",
    "If you can convert your problem to be answered by processing a function on a an array/matrix/tensor, then you can use DNNs to solve that problem\n",
    "\n",
    "It just so happens that deep learning is just 'that place' where this method is really needed.\n",
    "- Tensor flow is a Deep learning library becasue it has tons of Deep Learning functions and packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "How Tensorflow is setup:\n",
    "---\n",
    "\n",
    "1. Define your model in sort of abstract terms\n",
    "    - this is where you are building your \"computation graph\"\n",
    "\n",
    "2. When you're ready, you then 'run the session'\n",
    "    - that runs the graph and everything is done in the backend\n",
    "    - then you get your result back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are making the following constants to be very simple, but they can be `variables` or `placeholders`\n",
    "- We'll get into that later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1  = tf.constant(5)\n",
    "x2  = tf.constant(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow will recognize simplistic multiplication such as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = x1*x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We *might* be able to get away with this, but maybe not for a more complex model; and/or might not win a speed test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's not nearly as efficient as the official way of doing this, which uses `tf.mul`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = tf.mul(x1,x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for multiplying simple numbers together.  But there is a much more useful function for multiplying matrices, called `matmul`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "result = tf.matmul(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mul:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is an 'abstract' tensor in the graph `result`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the session, you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(result))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like a file, if you open it, you should close it \n",
    "-- like any connection object ever!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we should probably do everytime is something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `with`, much like with `file`, it will just open and close for you; so that you don't have to remember and/or won't forget to do so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to achieve this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    output = sess.run(result)\n",
    "    print(output)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that assigning the return struction from `sess.run` to a variable, stores that variable to a python variable, which can be manipulated later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Contrast, the following code segment will crash because we are accessing the tf.Session after it has both processed all of the data *and* been closed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    output = sess.run(result)\n",
    "    print(output)\n",
    "\n",
    "print(output)\n",
    "# print(sess.run(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del sess, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation graph is where you model everything; we build everything into it\n",
    "- nodes, layers, data, features, etc are in the graph\n",
    "\n",
    "Then we will run that graph to modify the weights iteratively\n",
    "- we have to tell tensorflow what data goes where and which cost functions to use, but overall it's behind the veil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With **Tensorflow**, you have do everything in 2 major chunks:\n",
    "1. Build the graph with all of the data and activation functions\n",
    "2. Run the graph to optimize the weights, which are everything that we are training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST Model Build Example\n",
    "===\n",
    "\n",
    "We are going to learn to format the data later, but for now we will work with the MNIST data set because it's been pre-engineered.\n",
    "\n",
    "After all of this, we will think about how to work with our own data sets\n",
    "\n",
    "MNIST: hand written digits\n",
    "---\n",
    "1. Train against 60k training digits\n",
    "2. Test against 10k testing digits\n",
    "3. Validate against 10k testing digits (after all training/testing is FINISHED!)\n",
    "\n",
    "Each 'feature' is a pixel value of either 0 or 1\n",
    "- on or off = is the pixel 'part of the digit' or 'just white space'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layout:\n",
    "\n",
    "Input data > weight > hidden layer 1 ( activation function ) > weights > hidden layer 2 ( activation function ) > weights > output layer\n",
    "\n",
    "Repeat this process up until convergence of output layer onto the training labels to optimize the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feed Forward:** Taking a neural net in one direction, from the input data, through *any* number of layers, into the output layer for comparison with known labels\n",
    "- Passes data 'straight through'\n",
    "- at the end, we compare the prediction output to the data\n",
    "- compute the cost or loss function (i.e. cross entropy)\n",
    "- use an optimizer to attemp to minimize that cost function\n",
    "    - we will use the AdamOptimizer\n",
    "    - there also exists **Stochastic Gradient Descent (SGD)**, AdaGrad, ..., ~8 options\n",
    "- the optimizer goes backwards through the NN to optimize the weights\n",
    "    -- this is called **back propagation (backprop)**\n",
    "- **Slang:** feed forward + backprop = **\"epoch\"**\n",
    "    - the algorithim will do this 15,20,50,... times\n",
    "    - hopefully each cycle, we will lower the cost function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('/tmp/data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`one hot encoding` comes from electronics, where only one component is `hot` or active and the rest are off\n",
    "    - if it physically has electricity running through, then NO others will be so\n",
    "\n",
    "We will then have 10 classes, from 0,1,2,...,9\n",
    "\n",
    "if you have a 0,1,2, ... digit, then normal labeling will result in 0,1,2,...\n",
    "\n",
    "But, for `one hot encoding`:\n",
    "0 = 1 0 0 0 0 0 0 0 0 0\n",
    "\n",
    "1 = 0 1 0 0 0 0 0 0 0 0\n",
    "\n",
    "2 = 0 0 1 0 0 0 0 0 0 0\n",
    "\n",
    "...\n",
    "\n",
    "8 = 0 0 0 0 0 0 0 0 1 0\n",
    "\n",
    "9 = 0 0 0 0 0 0 0 0 0 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "MNIST Example Solution\n",
    "---\n",
    "\n",
    "3 hidden layers, with 500, 500, 500 nodes. Depending on the \"thing you're trying to model\", the number and depth of each layers will vary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(mnist.train.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "\n",
    "n_classes   = 10\n",
    "\n",
    "n_pixels    = 784 # 28*28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of us will be able to load MNIST into our RAM, but that does not prepare us for the future\n",
    "- especially if it's common crawl, which will be pedabytes of ram\n",
    "- as a result, we set the `batch_size` to control the RAM usage\n",
    "- it also adds a level of randomness to the NN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we establish the data types and usage that TF is expecting. \n",
    "- with a matrix = Height x Width\n",
    "- we want to set the dimensions too. We will will flatten the picture into a 1D array using [None, size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Kenny GoodMan in the Youtube Comment section:\n",
    "\n",
    "```python\n",
    "def networks(data, nodes_and_layers):\n",
    "    length = len(nodes_and_layers) - 1\n",
    "    # create hidden layers\n",
    "    hiddenLayers = [0 for i in range(length)]\n",
    "    for i in range(length): \n",
    "         hiddenLayers[i] = {'weights': tf.Variable(tf.random_normal([nodes_and_layers[i],nodes_and_layers[i+1]])),\n",
    "                                          'biases' :tf.Variable(tf.random_normal(nodes_and_layers[i+1])) }\n",
    "         hiddenLayers[-1]['biases'] = tf.Variable(tf.random_normal([nodes_and_layers[-1]])) # change the biases of output\n",
    "      # relu(input_date * weights + biases)\n",
    "      layers = data\n",
    "      for i in range(length - 1):\n",
    "          layers = tf.nn.relu(tf.add(tf.matmul(layers, hiddenLayers[i]['weights']) + hiddenLayers[i]['biases']))\n",
    "      return tf.matmul(layers[-1], hiddenLayers[-1]['weights']) + hiddenLayers[-1]['biases']\n",
    "size_of_output = 10\n",
    "size_of_input = 784\n",
    "nodes_and_layers = [ size_of_input, 500, 500, 500, size_of_output ]\n",
    "networks(data, nodes_and_layers)﻿\n",
    "```\n",
    "\n",
    "From Edoardo Pona:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "n_nodes = [784, 500, 400, 300] #number of nodes, so that n_nodes[1] is n_nodes_hl1 etc.. the first value is the 784 that is needed at the beginning \n",
    "layers = []\n",
    "hiddenLayers = []\n",
    "n_layers = 100 #number of layers \n",
    "n_classes = 10\n",
    "\n",
    "def neural_network_model(data):\n",
    " layers.append(data) #the first is going to be the input data\n",
    " for i in xrange(len(n_nodes)-1):\n",
    "\n",
    "  if i < len(n_nodes-1):\n",
    "   hiddenLayers.append({'weights':tf.Variable(tf.random_normal([n_nodes[i], n_nodes[i+1]])),\n",
    "    'biases':tf.Variable(tf.random_normal(n_nodes[i]))})\n",
    "\n",
    "   layer=tf.add(tf.matmul(layers[i], hiddenLayers[i]['weights']) + hiddenLayers[i]['biases'])\n",
    "   layers.append[tf.nn.relu(layer)]\n",
    "\n",
    "  elif i == len(n_nodes-1):\n",
    "   output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes[i], n_classes])),\n",
    "     'biases':tf.Variable(tf.random_normal([n_clases]))}\n",
    "   output = tf.matmul(layers[i], output_layer['weights']) + output_layer['biases']\n",
    "   return output\n",
    "```\n",
    "\n",
    "From Santiago Penate:\n",
    "```python\n",
    "def neural_network_model2(data, input_nodes, layer_nodes, output_nodes):\n",
    "    \n",
    "    nodes = [input_nodes] + layer_nodes + [output_nodes]\n",
    "    \n",
    "    n_layers = len(layer_nodes) + 1\n",
    "    for i in range(n_layers):        \n",
    "        print('Layer', i, ':' )\n",
    "        print('\\tweights:', nodes[i], 'x', nodes[i+1])\n",
    "        print('\\tbiases:',  nodes[i+1])\n",
    "        w = tf.Variable(tf.random_normal([nodes[i], nodes[i + 1]]))\n",
    "        b = tf.Variable(tf.random_normal([nodes[i + 1]]))\n",
    "\n",
    "        # formula = data * weight + bias\n",
    "        if i == 0:\n",
    "            output = tf.add(tf.matmul(data, w), b)\n",
    "            output = tf.nn.relu(output)\n",
    "        else:\n",
    "            output = tf.add(tf.matmul(output, w), b)\n",
    "            if (i + 1) < n_layers:\n",
    "                output = tf.nn.relu(output)\n",
    "\n",
    "    return output\n",
    "\n",
    "neural_network_model2(x, input_size=28*28, layer_nodes=[500, 500, 500], output_size=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder('float', [None, 784])\n",
    "y = tf.placeholder('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you attempt to feed throug something that is **not** this shape, then TF will through an error.\n",
    "- Sometimes it can be useful to leave something in there because TF **won't** flag an error if you didn't specify the size\n",
    "\n",
    "X and Y are just placeholders for the graph to shove data through network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`biases` are something that is added through after the weigths are multiplied together\n",
    "- in the standard example above, we Summed all of the weights functions together\n",
    "- but it is better the pass \"(input_data * weights) + biases\n",
    "- with Rectilinear Units (ReLu) without biases, none of the neurons would EVER fire!\n",
    "\n",
    "`biases` make it so that the neuron will fire, even if the weights results in 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_feed_forward_neural_network_model(inputs, nOutputs, numberOfNeurons = []):\n",
    "    \n",
    "    if not isinstance(numberOfNeurons, list):\n",
    "        numberOfNeurons = [int(numberOfNeurons)]\n",
    "    \n",
    "    numberOfLayers = len(numberOfNeurons) # The number of layers is specified by the length of `numberOfNeurons`\n",
    "    if not numberOfLayers:\n",
    "        raise ValueError('`numberOfNeurons` must be a either an integer or list of integers')\n",
    "    \n",
    "    numberOfNeurons = np.int32(numberOfNeurons) # ensure they are integers\n",
    "    \n",
    "    hidden_layers   = [{'weights':None, 'biases':None}]*numberOfLayers\n",
    "    \n",
    "    numberOfNeurons = [inputs.shape[0]] + numberOfNeurons# + [nOutputs]\n",
    "    \n",
    "    # For loop over zip([all_but_the_last_layer, all_but_the_first_layer])\n",
    "    for nNeuronsNow, nNeuronsNext in zip(numberOfNeurons[:-1], numberOfNeurons[1:]):\n",
    "        hidden_layers[k]['weights'] = tf.Variable(tf.random_normal([nNeuronsNow, nNeuronsNext])),\n",
    "        hidden_layers[k]['biases']  = tf.Variable(tf.random_normal([nNeuronsNext]))\n",
    "    \n",
    "    output_layer   = {'weights':tf.Variable(tf.random_normal([numberOfNeurons[-1], nOutputs])),\n",
    "                      'biases': tf.Variable(tf.random_normal([nOutputs]))}\n",
    "    \n",
    "    # Setup the topology of the layers\n",
    "    layer = tf.add(tf.matmul(inputs, hidden_layers[k]['weights']) , hidden_layers[k]['biases'])\n",
    "    layers= [tf.nn.relu(layer)] # rectified linear activation function\n",
    "    for k in range(1,numberOfLayers):\n",
    "        layer = tf.add(tf.matmul(layers[-1], hidden_layers[k]['weights']) , hidden_layers[k]['biases'])\n",
    "        layers.append(tf.nn.relu(layer)) # rectified linear activation function\n",
    "    \n",
    "    # the output layer does *not* get summed all together, or get passed through the activation function\n",
    "    output = tf.add(tf.matmul(layers[-1], output_layer['weights']  ) , output_layer['biases'])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500, 500)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_feed_forward_neural_network_model(inputs, nOutputs, [n_nodes_hl1, n_nodes_hl2, n_nodes_hl3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_network_model(data):\n",
    "    # Shape must be `n_inputs` by `n_weights` or `n_pixels` by`n_nodes_hl1` here\n",
    "    # we start with random_normal to through so randomness into the beginning\n",
    "    # the biases are just the `vector` form of the weights\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([n_pixels, n_nodes_hl1])),\n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "    \n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "    \n",
    "    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "    \n",
    "    output_layer   = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                      'biases': tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "    layer1 = tf.add(tf.matmul(data, hidden_1_layer['weights']) , hidden_1_layer['biases'])\n",
    "    layer1 = tf.nn.relu(layer1) # rectified linear activation function\n",
    "    \n",
    "    layer2 = tf.add(tf.matmul(layer1  , hidden_2_layer['weights']) , hidden_2_layer['biases'])\n",
    "    layer2 = tf.nn.relu(layer2)\n",
    "    \n",
    "    layer3 = tf.add(tf.matmul(layer2  , hidden_3_layer['weights']) , hidden_3_layer['biases'])\n",
    "    layer3 = tf.nn.relu(layer3)\n",
    "    \n",
    "    # the output layer does *not* get summed all together, or get passed through the activation function\n",
    "    output = tf.add(tf.matmul(layer3  , output_layer['weights']  ) , output_layer['biases'])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    cost       = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction,y))\n",
    "    \n",
    "    # AdamOptimizer has a param called `learning_rate` wit default = 0.0001\n",
    "    optimizer  = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    nEpochs = 10\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        for epoch in range(nEpochs):\n",
    "            epoch_loss = 0\n",
    "            for _ in range(int(mnist.train.num_examples / batch_size)):\n",
    "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "                _, c  = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                epoch_loss += c\n",
    "            \n",
    "            print('Epoch', epoch, 'completed out of',nEpochs, 'loss:', epoch_loss)\n",
    "        \n",
    "        correct = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "        \n",
    "        accuracy= tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        \n",
    "        print('Accuracy:', accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch', 0, 'completed out of', 10, 'loss:', 1795090.7263946533)\n",
      "('Epoch', 1, 'completed out of', 10, 'loss:', 397719.79118537903)\n",
      "('Epoch', 2, 'completed out of', 10, 'loss:', 221768.89266777039)\n",
      "('Epoch', 3, 'completed out of', 10, 'loss:', 134885.91095066071)\n",
      "('Epoch', 4, 'completed out of', 10, 'loss:', 82654.702231878415)\n",
      "('Epoch', 5, 'completed out of', 10, 'loss:', 53040.316723240481)\n",
      "('Epoch', 6, 'completed out of', 10, 'loss:', 32520.759185434406)\n",
      "('Epoch', 7, 'completed out of', 10, 'loss:', 26949.754656583071)\n",
      "('Epoch', 8, 'completed out of', 10, 'loss:', 21321.228015989065)\n",
      "('Epoch', 9, 'completed out of', 10, 'loss:', 17755.359222649509)\n",
      "('Accuracy:', 0.94800001)\n"
     ]
    }
   ],
   "source": [
    "train_neural_network(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
